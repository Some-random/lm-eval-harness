Generate by calling "python async_openai_call_triviaqa_generate.py"
cp the output to lm-evaluation-harness/output/xx
go to output/xx, copy the directory from other models
For evaluative task: change the prompt, change the position that the evaluation file is read
cp the output to lm-evaluation-harness/output/xx_discriminate
run
